Важно понимать, что следует аккуратно подходить к выбору функции потерь и к регуляризации. 

Так Среднеквадратичная ошибка (MSE) штрафует большие ошибки сильнее, что делает ее чувствительной к выбросам. Средняя абсолютная ошибка (MAE) менее чувствительна к выбросам, чем MSE.

L1 штрафует абсолютные значения весов, что может приводить к тому, что некоторые веса могут оказаться равными нулю. Это позволяет убрать признаки, не вносящие существенного информационного вклада. L2 регуляризация штрафует большие значения весов, предотвращая их чрезмерный рост, но, в отличие от L1, не загуляет их полностью.

На основании проведенного эксперимента можно сделать вывод, что самописная модель по качеству близка к модели из sklearn. Нам удалось достигнуть MSE 46.887851136687495 против 42.53541245128315 у модели из sklearn. Весьма вероятно, что ошибку самописной модели можно было бы еще сильнее уменьшить, если бы модель учитывала bias.

В целом работа показывает, что линейная регрессия может быть эффективно использована для некоторых типов задач.